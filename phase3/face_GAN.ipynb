{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLVWwcoyK40r"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wGD2Dz61nb7",
        "outputId": "ec924eef-1841-4016-bbea-cfd5b8599155"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'celeba-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F29561%2F37705%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240724%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240724T082317Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7cc28cbdcd2daa3091b54b5a3d8d68afb3294802aa09d33e1dc30689e79b339de98d830b2fe77afed474042ac6551e22617fe4cddf13478ec62225c4bb2f99a6868100a71cab411607aec35b6faf412814a5dad8845a081e98d02b43cb9f852c923c4ee6e41f3afbee5e80e0f9843bbba98277ece35c195bf893cf7740caa8897ea85939d6b6604dd084127c83a9a59104bfefd51bd0cc5a4ae88c81ffa88039b199d84933ff397d58b6293e57b99be9b71f5f833da9454e92aeb4ea7337f84e8ef971eedb499727cdbb6f60e5d95f976447570ae5e281a0e07d36e978900dd2d1f4b8d0aec462ee5c6fee648b7404d105834672f2eed024ce9a178466812c96'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "AoN3gna31ncF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image as Img\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL0CgoPW1ncG"
      },
      "source": [
        "### Loading the data and resizing the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFtwKhXs1ncH",
        "outputId": "5f9541b2-532b-4c9a-e4ff-3bc347623578",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "PIC_DIR = f'../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n",
        "\n",
        "IMAGES_COUNT = 10000\n",
        "\n",
        "ORIG_WIDTH = 178\n",
        "ORIG_HEIGHT = 208\n",
        "diff = (ORIG_HEIGHT - ORIG_WIDTH) // 2\n",
        "\n",
        "WIDTH = 128\n",
        "HEIGHT = 128\n",
        "\n",
        "crop_rect = (0, diff, ORIG_WIDTH, ORIG_HEIGHT - diff)\n",
        "\n",
        "images = []\n",
        "for pic_file in tqdm(os.listdir(PIC_DIR)[:IMAGES_COUNT]):\n",
        "    pic = Img.open(PIC_DIR + pic_file).crop(crop_rect)\n",
        "    pic.thumbnail((WIDTH, HEIGHT), Img.ANTIALIAS)\n",
        "    images.append(np.uint8(pic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bpMJ7nA1ncI",
        "outputId": "46c03f3f-3ac1-4ce2-c50e-a5a463e2dc8b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Image shape\n",
        "images = np.array(images) / 255\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY9ukQPO1ncJ"
      },
      "source": [
        "### Displaying the first 25 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "I8vR_J_L1ncK",
        "outputId": "d2e01529-be46-485a-c76b-582a548332e3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(1, figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL9waaFv1ncK"
      },
      "source": [
        "### Defining the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZchoLJ0R1ncK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "LATENT_DIM = 32\n",
        "CHANNELS = 3\n",
        "\n",
        "def create_generator():\n",
        "    gen_input = Input(shape=(LATENT_DIM, ))\n",
        "\n",
        "    x = Dense(128 * 16 * 16)(gen_input)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Reshape((16, 16, 128))(x)\n",
        "\n",
        "    x = Conv2D(256, 5, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2D(512, 5, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(512, 5, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(CHANNELS, 7, activation='tanh', padding='same')(x)\n",
        "\n",
        "    generator = Model(gen_input, x)\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0WPRnaV1ncK"
      },
      "source": [
        "### Defining the Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irrttfrv1ncL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_discriminator():\n",
        "    disc_input = Input(shape=(HEIGHT, WIDTH, CHANNELS))\n",
        "\n",
        "    x = Conv2D(256, 3)(disc_input)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2D(256, 4, strides=2)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2D(256, 4, strides=2)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2D(256, 4, strides=2)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv2D(256, 4, strides=2)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "    discriminator = Model(disc_input, x)\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=.0001,\n",
        "      decay_steps=10000,\n",
        "      decay_rate=1e-8\n",
        "    )\n",
        "    optimizer = RMSprop(learning_rate=lr_schedule, clipvalue=1.)\n",
        "\n",
        "    discriminator.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy'\n",
        "    )\n",
        "\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNujLvm1ncL"
      },
      "source": [
        "### Defining the GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmYFDQEW1ncL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from tensorflow.keras.utils import model_to_dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCC805tY1ncL",
        "outputId": "84049b76-7a44-46ae-cd7d-8e319027f9be",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "generator = create_generator()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fKbORt8s1ncM",
        "outputId": "3d398eca-1a06-4aaf-d8d8-f407825c7c4a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Image(model_to_dot(generator, show_shapes=True).create_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYhKTDzo1ncM",
        "outputId": "538be6d7-40c7-42e3-eb7a-16ed9b21ece3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "discriminator = create_discriminator()\n",
        "discriminator.trainable = False\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4mjSzZjo1ncM",
        "outputId": "cbc7a115-0102-4604-9935-725dfaee09ce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Image(model_to_dot(discriminator, show_shapes=True).create_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw7cMZQU1ncM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "gan_input = Input(shape=(LATENT_DIM, ))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZEFpOyL1ncM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#optimizer = RMSprop(lr=.0001, clipvalue=1.0, decay=1e-8)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=.0001,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=1e-8\n",
        ")\n",
        "optimizer = RMSprop(learning_rate=lr_schedule, clipvalue=1.)\n",
        "gan.compile(optimizer=optimizer, loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3-2KuV-1ncN",
        "outputId": "b2e9bfa6-1bfe-4f02-acad-dcfad9e9ced9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "gan.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_E0BMef1ncN"
      },
      "source": [
        "### Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i31u7T721ncN",
        "outputId": "8cb85ecb-7dbe-4caf-e3b4-a7f435b29ee1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "iters = 15000\n",
        "batch_size = 16\n",
        "\n",
        "RES_DIR = 'res2'\n",
        "FILE_PATH = '%s/generated_%d.png'\n",
        "if not os.path.isdir(RES_DIR):\n",
        "    os.mkdir(RES_DIR)\n",
        "\n",
        "CONTROL_SIZE_SQRT = 6\n",
        "control_vectors = np.random.normal(size=(CONTROL_SIZE_SQRT**2, LATENT_DIM)) / 2\n",
        "\n",
        "start = 0\n",
        "d_losses = []\n",
        "a_losses = []\n",
        "images_saved = 0\n",
        "for step in tqdm(range(iters)):\n",
        "    start_time = time.time()\n",
        "    latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n",
        "    generated = generator.predict(latent_vectors, verbose=0)\n",
        "\n",
        "    real = images[start:start + batch_size]\n",
        "    combined_images = np.concatenate([generated, real])\n",
        "\n",
        "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "    labels += .05 * np.random.random(labels.shape)\n",
        "\n",
        "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "    d_losses.append(d_loss)\n",
        "\n",
        "    latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n",
        "    misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "    a_loss = gan.train_on_batch(latent_vectors, misleading_targets)\n",
        "    a_losses.append(a_loss)\n",
        "\n",
        "    start += batch_size\n",
        "    if start > images.shape[0] - batch_size:\n",
        "        start = 0\n",
        "\n",
        "    if step % 50 == 49:\n",
        "        gan.save_weights('/gan.h5')\n",
        "\n",
        "        print('%d/%d: d_loss: %.4f,  a_loss: %.4f.  (%.1f sec)' % (step + 1, iters, d_loss, a_loss, time.time() - start_time))\n",
        "\n",
        "        control_image = np.zeros((WIDTH * CONTROL_SIZE_SQRT, HEIGHT * CONTROL_SIZE_SQRT, CHANNELS))\n",
        "        control_generated = generator.predict(control_vectors, verbose=0)\n",
        "\n",
        "        for i in range(CONTROL_SIZE_SQRT ** 2):\n",
        "            x_off = i % CONTROL_SIZE_SQRT\n",
        "            y_off = i // CONTROL_SIZE_SQRT\n",
        "            control_image[x_off * WIDTH:(x_off + 1) * WIDTH, y_off * HEIGHT:(y_off + 1) * HEIGHT, :] = control_generated[i, :, :, :]\n",
        "        im = Img.fromarray(np.uint8(control_image * 255))#.save(StringIO(), 'jpeg')\n",
        "        im.save(FILE_PATH % (RES_DIR, images_saved))\n",
        "        images_saved += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3tpBE4q1ncN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(1, figsize=(12, 8))\n",
        "plt.subplot(121)\n",
        "plt.plot(d_losses, color='red')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('discriminant losses')\n",
        "plt.subplot(122)\n",
        "plt.plot(a_losses)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('adversary losses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOSTogdt1ncO"
      },
      "source": [
        "### Let us also make the GIF of the output images that have been generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAljble81ncO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import shutil\n",
        "images_to_gif = []\n",
        "for filename in os.listdir(RES_DIR):\n",
        "    images_to_gif.append(imageio.imread(RES_DIR + '/' + filename))\n",
        "imageio.mimsave('trainnig_visual.gif', images_to_gif)\n",
        "shutil.rmtree(RES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6J52csc1ncO"
      },
      "source": [
        "Check the output here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VkibnBi1ncO"
      },
      "source": [
        "https://giphy.com/gifs/QuDO8a4UBfyZrLP331?utm_source=iframe&utm_medium=embed&utm_campaign=Embeds&utm_term=https%3A%2F%2Fwww.kdnuggets.com%2F"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
